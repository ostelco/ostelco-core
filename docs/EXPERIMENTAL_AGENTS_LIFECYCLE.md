Experimental agents
===

Agents and the context they work in
----
This directory contains a set of experiments, that should eventually
lead to a framework, a very simple one, for making analytic agents.

Analytic agents are in this context defined as processes, either
fully automatic or partly manual, that takes data about user
behavoir and demographics, and translates this into offers
that can be presented to subscribers to be acted on.

     demographics+behavior -> agent -> offer to subscriber.

It is important to note that even though it does make sense
for agents to run only once, it makes even more sense if they
run more than once, meaning that future runs of agents will
be observing the results of previous actions.  This makes
puts the agents in a position to be learning agents.

Interaction with analytics systems
----
There are already many analytics systems in this world, google
analytics, firebase analytics, kissmetrics, ... . Our analytics agent
subsystem should emphaticly _not_ replicate much or anything
of what these existing systems do.   Reports, ad-hoc queries,
dashboards, setting up funnels, tracking campaigns etc.  They all do
this much better than we can hope for, so  we should use them, not
copy them.   What we _do_ want is to be able to work in concert with
them.   We want to be able to pick up events that are generated on the
basis of offers generated by our analytics agents, and track them
using external analytics tools, and to use the same terms as the
tools we are working in concert with.  This means that we need
to solve a coordination problem with respect to whatever analytics
system we choose to use.   Our current working hypothesis is that we
will be using Firebase Analytics,


Current overall design
---


### Consumption cycle

The current overall design is what we think today should be the
architecture to aim for.    This is a moving target, tomorrow
it could be something else.  With that caveat in mind, here goes:

We set up a processing cycle with these steps and formats.

* Continously collect data using whatever means necessary, and dump
  them into bigquery.  Data is stored pseudoanonymized.  The
  bigquery dumps should contain at least: Data consumption data and
  behavior data fra analytics.

Periodically, every hour or so, run a kubernetes cron job that does this:

*  Run predefined queries from the data stored in bigquery,
   extract tables for demoraphics, etc. that makes sense
   for the analytics agents to work with.  Not too much,
   not too little, subject to change and the models must
   allow for this (e.g. require that columns must be
   allowed to appear without consumers breaking).
   Put the results into temporary bigquery tables.

* Translate from pseudoanonymized data into consistent
  pseudoanonymized datasets.

* Dump the translated & consistent dataset into one or more cloud
  storage buckets, for consumption by agents.

Agents can then either

* Read the data directly from the bucket programmatically
* Read the data form the bucket via the web inteface, and then
  process it from local file storage.
* Use FUSE filesystem to read data from bucket, but present it
  as a local file system.

 * Agents will then take the input, process it, produce
   instructions for offers to be made. Do this in the form of
   yaml files (or perhaps a single yaml file, see sample
   format in this directory).  The output is written
   back into a cloud storage bucket.

The output from the agent is then picked up by a job
listening for input into buckets (either by running
periodically, or by active listening for changes).
The offers are then translated into internal format
offers, written into the appropriate databases,
possibly signalled through messaging services and
 immediately picked up by the subscribers.

This completes the cycle.

### Components

A kubernetes program running either continously, or as a kubernets
batch job that exports and imports datasets via google cloud storage.
Written in Kotlin, running as a dropwizard job.

The component should use a pubsub channel to listen to changes in the
actual changes in the bucket. The reason for favoring pubsub, is that
it is possible to run tests of the component on a workstation making
it unecessary to deploy the comonent to a production-like environment
to run it in an environment that is _almost_ production like but
runs on a workstation.

### Example yaml file describing offer (work in progress)

This version is based on the data we used in a previous version of the
"prime" component's internal schema, to which is added some fields
that are assumed to make sense when describing offers.  This proposal
has not been made as minimal as possible, and it has not been
critisized to death (a.k.a. "code reviewed" :) ) to ensure that it is
the best possible format we can make today.  That should happen before
we commit to using it, or write even a single line o code to parse or
produce it!


	#
	# This is a sample YAML format to be used by
	# agents that produce offers.  The general idea
	# is that an offer has a set of parameters,
	# and also a set of selected subscribers that will
	# get it.
	#
	# YAML was chosen since it's more human readable than
	# e.g. json or protobuffers, while still being
	# easy to produce by an agent, and relatively compact,
	# in particular when gzipped.
	#

	producing-agent:
	  name: Simple agent
	  version: 1.0

	# All of the parameters below are just copied from the firebasr
	# realtime database we used in the demo, converted to
	# camel case.  All the fields should be documented
	# in this document, and we should think through if this is
	# the best set of parameters we weant.

	offer:
		history:
		   createdAt: "2018-02-22T12:41:49.871Z"
		   updatedAt: "2018-02-22T12:41:49.871Z"
		   visibleFrom: "2018-02-22T12:41:49.871Z"
		   expiresOn: "2018-02-22T12:41:49.871Z"
		presentation:
		   badgeLabel: "mbop"
		   description: "Best offer you will get today"
		   shortDescription: "Best offer!"
		   label: "3 GB"
		   name: "3 GB"
		   priceLabel: "49 NOK"
		   hidden: false
		   image: https://www.ft-associates.com/wp-content/uploads/2015/08/Best-Offer.jpg
		financial:
		   repurchability:1
		   currencyLabel: "NOK"
		   price: 4900
		   taxRate: 10
		product:
		   SKU: 2
		   # A possibly very long list of product parameters that are all
		   # dependent on the SKU's requirement.   Details ignored here,
		   # that may pop up later. Deal with them then.
		   noOfBytes: 3000000000

	# We put the segment last, since it may have a long list of
	# members in it. We want that list to be last, since it contains
	# little information that humans are interested in, and we want
	# humans to start reading the file at the top.

	segment:
	   type: agent-specific-segment
	   members:
		 # The decryption key is what the de-anonymizer will use to
		 # make proper  identifiers out of the members listed below.
		 # The special purpose key "none" indicatest that the member list
		 # is in clear text.
		 decryptionKey: none
		 members:
			- 4790300157
			- 4790300144
			- 4333333333




Notes
---

### Some questions about the bigquery structure:

* We should use timestamps I think, since it's simpler to work with.

* We should also considere using ingestion-time partitioned tables

   https://cloud.google.com/bigquery/docs/creating-partitioned-tables

* Most of our time-dependent queries will overlap reasonably
      sized shards (days, weeks, months), so that we don't have to
      table-scan _all_ of the potentially very large datasets we will
      accumulate.

* Don't name any field 'timestamp'. It leads to endless confusion
   with the datatype named 'timestamp', and the function called
   'timestamp'.

### Formatting guidelines

* We should decide if we will use camel case or snake case in
   offer descriptions. The current firebase/firestore structure uses
   both, and that is confusing.

### Accessing data via FUSE

* To get a FUSE mounted cloud storage on a mac, do this:
   https://cloud.google.com/storage/docs/gcs-fuse

* Download from
   https://github.com/GoogleCloudPlatform/gcsfuse/
   https://github.com/GoogleCloudPlatform/gcsfuse/blob/master/docs/installing.md

* Install this way

       brew install gcsfuse
       sudo ln -s /usr/local/sbin/mount_gcsfuse /sbin  # For mount(8) support

* Create certficate to make fuse authenticate correctly
Make a certificate https://console.cloud.google.com/apis/credentials
To authenticate

     export GOOGLE_APPLICATION_CREDENTIALS=/some/path/prime-service-account.json

then

    gcsfuse rmz-test-bucket /tmp/aaas



## Installing google cloud comman dline interface

https://cloud.google.com/sdk/docs/

To authenticvate Authenticate

   gcloud auth login

(will use the browser to do actual authentication)


Set your project:

   gcloud config set project GCP_PROJECT_ID

.. or some other project

Run a script to get some data, e.g.


    bq head -n 10 $GCP_PROJECT_ID:data_consumption.hourly_consumption

to get ten lines of consumption data displayed.


TODO
===

* Disuss with Cecilie etc. what's really necessary.
* Write kotlin/kubernetes program that will extract data from
  misc. sources and present for agent consumption/read data from
  agents.
* Declare initial victory, but be prepared for massive rewrites when
  actual experience from using these tools become available
